{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "958be343",
   "metadata": {},
   "source": [
    "# DSCoV Napari Workshop\n",
    "\n",
    "Welcome to DSCoV! Today we are going to explore how to use Napari to view 3D volumetric images and visualize the results of image segmentation. We will also learn how to customize the GUI of Napari for your specific tasks.\n",
    "\n",
    "Please feel free to execute the cells in this notebook and follow along.\n",
    "\n",
    "Special thanks to **Dr. Alexander Fleischmann** and **Sara Zeppilli** for providing the 3D volumetric images in their research as examples for this workshop!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b73b64b7",
   "metadata": {},
   "source": [
    "# Task: Visualizing Image Segmentation results\n",
    "\n",
    "## Task description\n",
    "\n",
    "We have 3D microscopic scans of mice brains at different developmental stages. They have been cleared and stained with different antibiotics to reveal different cell nuclei in the allocortex and neocortex regions in the brain. We have run image segmentation to identify the cell nuclei. Now we need to visualize the prediction and compare it with the ground truth to understand how well the segmentation performs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98d25db0",
   "metadata": {},
   "source": [
    "## Load package and images\n",
    "\n",
    "First, we load the `napari` package and others used in this tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d10c77e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the necessary packages\n",
    "\n",
    "import napari\n",
    "import numpy as np\n",
    "from skimage.io import imread"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e913f7ee",
   "metadata": {},
   "source": [
    "Then we load the image, ground truth, and prediction into memory as numpy arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "258bcc09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The image is a 3D 16-bit tif file which we will load with `imread` from scikit-image\n",
    "brain_image = imread(\"images/brain_image.tif\")\n",
    "\n",
    "# Ground truth and predictions are just numpy arrays saved in .npy format\n",
    "ground_truth = np.load(\"images/ground_truth.npy\").astype(int)\n",
    "prediction = np.load(\"images/prediction.npy\").astype(int)\n",
    "\n",
    "print(\"Shape of image: \", brain_image.shape)\n",
    "print(\"Shape of ground truth: \", ground_truth.shape)\n",
    "print(\"Shape of prediciton: \", prediction.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e6cb0ba",
   "metadata": {},
   "source": [
    "## View the 3D volumetric image\n",
    "\n",
    "We can pass the loaded image to `napari.view_image()`. A window will pop up with the image.\n",
    "\n",
    "Under the hood, this function creates an `image` layer in a `Viewer` object which we can manipulate later to add more layers. Any changes we make to this `Viewer` object will be reflected in the napari window immediately. Let's take a look at the window itself.\n",
    "\n",
    "![napari viewer window](https://napari.org/stable/_images/viewer_layout.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b50e4b5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "viewer: napari.Viewer = napari.view_image(brain_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd2a9af3",
   "metadata": {},
   "source": [
    "This image stack is visualized in the **canvas** region of the viewer window. You can use the **dimension sliders** to view each slice on the z-axis one after another.\n",
    "\n",
    "The six viewer buttons that you can use:\n",
    "* **IPython console**: if you launch Napari from command line, you wil have an interactive IPython console to interact with the `viewer` object in memory.\n",
    "* **ndview**: toggle 2D/nD view of the image\n",
    "* **Change order of axes**: view the image from different angles\n",
    "* **Transpose**: transpose (rotate) the image\n",
    "* **Grid view**: view layers separately in a grid\n",
    "* **Reset view**: reset all changes and go back to the original state\n",
    "\n",
    "Feel free to use different buttons and see what happens. You can always use the last one to go back to the original state."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26ab614e",
   "metadata": {},
   "source": [
    "## Adding layers\n",
    "\n",
    "The most important concept in `napari` is `layers`. You can add different types of layers on top of each other. There are 7 types of layers:\n",
    "\n",
    "* `image` layer: Displays an image. [More info](https://napari.org/stable/howtos/layers/image.html)\n",
    "* `labels` layer: An array of type `int`. Often used to display segmentation results, with different `int` values representing the class of the object in each segment. [More info](https://napari.org/stable/howtos/layers/labels.html)\n",
    "* `points` layer: An N x D array with `n` points in `d` dimensions. [More info](https://napari.org/stable/howtos/layers/points.html)\n",
    "* `shapes` layer: A list of `k` N x D arrays with `k` shapes of `n` points in `d` dimensions. [More info](https://napari.org/stable/howtos/layers/shapes.html)\n",
    "* `surface` layer: A generalization of shapes layer to display polygons. [More info](https://napari.org/stable/howtos/layers/surface.html)\n",
    "* `tracks` layer: used to describe tracks of objects across time steps. [More info](https://napari.org/stable/howtos/layers/tracks.html)\n",
    "* `vectors` layer: used to display vectors. [More info](https://napari.org/stable/howtos/layers/vectors.html)\n",
    "\n",
    "We can use `viewer.add_*()` function to add a layer to the viewer. This method will return an object of `Layer` type which you can modify later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de3a5f43",
   "metadata": {},
   "outputs": [],
   "source": [
    "gt_layer = viewer.add_labels(ground_truth)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4548bc5",
   "metadata": {},
   "source": [
    "Now that we have added a layer to the viewer. However, the layer does not look too good right now. We can customize the layer with **layer controls** on the left, once the layer is selected in the **layer list**. However, we can also modify the layers programmatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8abce53e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's create a dict that maps certain values in the layer to a set color.\n",
    "\n",
    "colormap = {\n",
    "    1: (0, 1, 0, 0.55) # An RGBA 4-tuple\n",
    "}\n",
    "\n",
    "gt_layer.color = colormap\n",
    "gt_layer.name = \"Ground Truth\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0fc22f8",
   "metadata": {},
   "source": [
    "We can also add all these settings while adding the layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea04a854",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_layer = viewer.add_labels(prediction, name=\"Predictions\", color={1: (1, 0, 1, 0.5)})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f4ea46b",
   "metadata": {},
   "source": [
    "## Customizing the interface\n",
    "\n",
    "One of the biggest advantage of `napari` is its flexibility and extensibility. You can customize its GUI to add buttons, controls, and have it respond to custom mouse, keyboard events. Let us begin with a simple one: key binding.\n",
    "\n",
    "### Event handling in `napari`: key binding\n",
    "\n",
    "This is done through a decorator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99327ced",
   "metadata": {},
   "outputs": [],
   "source": [
    "@viewer.bind_key(\"d\")\n",
    "def delete_layer(viewer): # The function being decorated accepts a Viewer object as argument\n",
    "    deleted_layer = viewer.layers.pop(-1)\n",
    "    \n",
    "    viewer.status = f\"Layer {deleted_layer.name} removed.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc6ead78",
   "metadata": {},
   "source": [
    "### We can also have napari respond to mouse events\n",
    "\n",
    "`napari` supports various mouse events from clicking, double clicking, to dragging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7b0b0ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "empty_layer = viewer.add_image(np.zeros(brain_image.shape), name=\"random noise\")\n",
    "\n",
    "@empty_layer.mouse_drag_callbacks.append\n",
    "def randomize(layer, event):\n",
    "    layer.data = np.random.random(brain_image.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b70f64fc",
   "metadata": {},
   "source": [
    "### Now, let's try adding additional functionalities to Napari\n",
    "\n",
    "It seems that the image is a bit too dark. Aside from adjusting the `contrast_limit` option, we can also use any arbitrary Python image processing library to process the image. Here we are going to use `scikit-image` to perform automatic histogram based adjustments to enhance our image. There are both global and local adjustment options available to us. For details on these adjustments, please check [this page](https://scikit-image.org/docs/stable/auto_examples/color_exposure/plot_local_equalize.html#sphx-glr-auto-examples-color-exposure-plot-local-equalize-py).\n",
    "\n",
    "`Napari` uses `magicgui` for customizing its GUI. `magicgui` checks the type annotations of the function being decorated to determine which control to show in the widget.\n",
    "\n",
    "In the next example, we use type annotations of the parameter types and return types of `adjustment_widget` function to tell `magicgui` what to show in the widget dock.\n",
    "\n",
    "``` python\n",
    "@magicgui(\n",
    "    call_button=\"Apply threshold\",\n",
    "    adjustment={\"widget_type\": \"RadioButton\", \"choices\": [\"global\", \"local\"]},\n",
    "    radius={\"widget_type\": \"Slider\", \"max\": 10}\n",
    ")\n",
    "def adjustment_widget(\n",
    "    image: napari.layers.Image, \n",
    "    adjustment: str=\"global\", \n",
    "    radius:int=3\n",
    ") -> napari.layers.Image:\n",
    "```\n",
    "\n",
    "* `radius` is `int` type, we tell `magicgui` to display a slider to choose the value for `radius`\n",
    "* `adjustment` is `str` type. By default, `magicgui` will display a textbox. However, what we want is a set of radio buttons, so we specify the `widget_type` to be `RadioButton` with two choices.\n",
    "* You can use `napari` builtin types too. Here we are using `Image`, which is a subclass of `Layer` type. Napari will show a dropdown of all layers of `Image` type. The argument passed to the function will be a reference to that layer.\n",
    "* We also specify the return type of the function. If the return type is a subclass of `Layer`, then `napari` will add this layer to the viewer.\n",
    "* `call_button` parameter to the decorator produces a button. Once clicked, it will execute the function, passing all values in the widget to the function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39cfc559",
   "metadata": {},
   "outputs": [],
   "source": [
    "from magicgui import magicgui\n",
    "from skimage import exposure\n",
    "from skimage.morphology import ball\n",
    "from skimage.filters import rank\n",
    "\n",
    "@magicgui(\n",
    "    call_button=\"Apply enhancement\",\n",
    "    adjustment={\"widget_type\": \"RadioButton\", \"choices\": [\"global\", \"local\"]},\n",
    "    radius={\"widget_type\": \"Slider\", \"max\": 10}\n",
    ")\n",
    "def adjustment_widget(\n",
    "    image: napari.layers.Image, \n",
    "    adjustment: str=\"global\", \n",
    "    radius:int=3\n",
    ") -> napari.layers.Image:\n",
    "    \n",
    "    data = image.data # Extract the data from the layer\n",
    "    \n",
    "    if adjustment == \"global\":\n",
    "        new_data = exposure.equalize_hist(data)\n",
    "    elif adjustment == \"local\":\n",
    "        neighborhood = ball(radius)\n",
    "        new_data = rank.equalize(data, footprint=neighborhood)\n",
    "        \n",
    "    return napari.layers.Image(new_data, name=f\"{adjustment} adjusted\")\n",
    "\n",
    "# Register this widget to the viewer\n",
    "viewer.window.add_dock_widget(adjustment_widget)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab15df88",
   "metadata": {},
   "source": [
    "A \"Dock Widget\" will show up. Now pick an adjustment (global or local), and if you choose local adjustment, choose a radius of the neighborhood (this might be slow), then click **Apply enhancement**.\n",
    "\n",
    "You can run any arbitrary code within the `adjustment_widget` function. That means you can do absolutely anything with your image. For example, you can run an end-to-end image segmentation/object detection network and have the results visualized immediately. Even better, once you have established a workflow, you can publish it as a `napari` plugin. You can install plugins directly from the `plugins` menu. Or you can go to [napari hub](https://www.napari-hub.org/) to download more cutting-edge plugins."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "295d1947",
   "metadata": {},
   "source": [
    "## Use `napari` to view large volumes that do not fit into memory\n",
    "\n",
    "Volumetric images are usually so large that they do not fit into the memory of a single computer. `napari` works not only with `numpy` arrays, it also works with arrays in library such as `dask`, `zarr`, and `xarray`, as long as they implement an `asarray()` method that turns their data structures into `numpy` arrays.\n",
    "\n",
    "* `dask` is a library used to perform out-of-memory computation on large data.\n",
    "* `zarr` is a storage backend to store chucked arrays that can be compressed.\n",
    "* `xarray` provides support for labeled multi-dimensional arrays often used in Bayesian analyses.\n",
    "\n",
    "With `dask-image`, viewing very large multi-dimensional images can be very simple. In the above example, we used `skimage.io.imread` to read the image. `dask-image` also provides an `imread` function to lazily load images in chunks, which can then be fed to `napari`:\n",
    "\n",
    "``` python\n",
    "from dask_image import imread\n",
    "image = imread(\"<path>/*.tif\") # Assuming each slice is a numbered tif\n",
    "\n",
    "viewer = napari.view_image(image)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5edcea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask_image.imread\n",
    "import dask.array as da\n",
    "\n",
    "# Lasily load the image with dask_image\n",
    "very_large_image = dask_image.imread.imread(\"/gpfs/data/datasci/yxu150/projects/cv-segment-nuclei/data/full_images/stitch 171211_P1-Ctip2_ENS/Stitched Image_*.tif\")\n",
    "very_large_label = da.from_zarr(\"/gpfs/data/datasci/yxu150/projects/cv-segment-nuclei/data/full_labels/CTIP2_P1/candidate_2.zarr\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "677081a6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "very_large_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c23690c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "very_large_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd79db8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "large_image_viewer = napari.view_image(\n",
    "    very_large_image, \n",
    "    contrast_limits=[0, 5000]\n",
    ")\n",
    "large_label_layer = large_image_viewer.add_labels(\n",
    "    very_large_label > 0, \n",
    "    name=\"Predictions\", \n",
    "    color={True: (0, 1, 0, 0.5)}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88f0dc76",
   "metadata": {},
   "source": [
    "# Bonus: Use `Napari` with Dall-E\n",
    "\n",
    "OpenAI just published their Dall-E API. We can use this API to write a Dall-E image generation interface in `Napari`. Here's a very simple example. You can register at OpenAI to get your own API key to play with it yourself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79afe122",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "\n",
    "openai.api_key = os.getenv(\"OPENAI_KEY\") # Replace this with your own key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22d6750f",
   "metadata": {},
   "outputs": [],
   "source": [
    "viewer = napari.view_image(np.zeros((1024, 1024, 3)), name=\"Generated Image\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d26078f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "@magicgui(\n",
    "    call_button=\"Generate Image\"\n",
    ")\n",
    "def generate_image(image_data: napari.types.ImageData, prompt:str) -> napari.types.ImageData:\n",
    "    if image_data is not None and prompt != \"\":\n",
    "        response = openai.Image.create(\n",
    "            prompt=prompt,\n",
    "            size=\"1024x1024\",\n",
    "            n=1\n",
    "        )\n",
    "        return imread(response[\"data\"][0][\"url\"])\n",
    "    \n",
    "viewer.window.add_dock_widget(generate_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85718a92",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
